{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a7c9df26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\upbjvd\\appdata\\local\\anaconda3\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\upbjvd\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "298d1a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d2b185b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all CSV files in the \"Hist_Data\" folder\n",
    "file_list = glob.glob('test folder/*.csv')\n",
    "\n",
    "# Get the total number of files\n",
    "total_files = len(file_list)\n",
    "\n",
    "with open('Pseudonym_Table/timeseries_mapping.pkl', 'rb') as f:\n",
    "    timeseries_mapping = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "3fd1362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time range\n",
    "train_1_start = '2013-01-01 00:15:00'\n",
    "train_1_end = '2017-07-01 00:00:00'\n",
    "test_1_start = '2017-07-01 00:15:00'\n",
    "test_1_end = '2018-07-01 00:00:00'\n",
    "train_2_start = '2018-07-01 00:15:00'\n",
    "train_2_end = '2022-07-01 00:00:00'\n",
    "c_start = '2022-07-01 00:15:00'\n",
    "test_2_end = '2023-05-30 23:45:00'\n",
    "\n",
    "# Create an empty DataFrame with a date range\n",
    "train_1_date_range = pd.date_range(start=train_1_start, end=train_1_end, freq='15T')\n",
    "train_1_empty_df = pd.DataFrame(index=train_1_date_range)\n",
    "train_1_empty_df.index.name = 'Zeitstempel'\n",
    "train_1_empty_df = train_1_empty_df.reset_index()\n",
    "\n",
    "test_1_date_range = pd.date_range(start=test_1_start, end=test_1_end, freq='15T')\n",
    "test_1_empty_df = pd.DataFrame(index=test_1_date_range)\n",
    "test_1_empty_df.index.name = 'Zeitstempel'\n",
    "test_1_empty_df = test_1_empty_df.reset_index()\n",
    "\n",
    "train_2_date_range = pd.date_range(start=train_2_start, end=train_2_end, freq='15T')\n",
    "train_2_empty_df = pd.DataFrame(index=train_2_date_range)\n",
    "train_2_empty_df.index.name = 'Zeitstempel'\n",
    "train_2_empty_df = train_2_empty_df.reset_index()\n",
    "\n",
    "test_2_date_range = pd.date_range(start=test_2_start, end=test_2_end, freq='15T')\n",
    "test_2_empty_df = pd.DataFrame(index=test_2_date_range)\n",
    "test_2_empty_df.index.name = 'Zeitstempel'\n",
    "test_2_empty_df = test_2_empty_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "4ba1512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statistics(dataframe):\n",
    "    if dataframe is df:\n",
    "        dataset = 'Dataset' + '_' +  str(processed_files)\n",
    "        filename = 'Dataset_' + alphabetic_id + '_' +  str(processed_files)\n",
    "    elif dataframe is train_1_original:\n",
    "        dataset = 'train_1' + '_' +  str(processed_files)\n",
    "        filename = 'train_1_' + alphabetic_id + '_' +  str(processed_files)\n",
    "    elif dataframe is test_1_original:\n",
    "        dataset = 'test_1' + '_' +  str(processed_files)\n",
    "        filename = 'test_1_' + alphabetic_id + '_' +  str(processed_files)\n",
    "    elif dataframe is train_2_original:\n",
    "        dataset = 'train_2' + '_' +  str(processed_files)\n",
    "        filename = 'train_2_' + alphabetic_id + '_' +  str(processed_files)\n",
    "    elif dataframe is test_2_original:\n",
    "        dataset = 'test_2' + '_' +  str(processed_files)\n",
    "        filename = 'test_2_' + alphabetic_id + '_' +  str(processed_files)\n",
    "    \n",
    "    # Number of rows\n",
    "    num_rows = len(dataframe)\n",
    "\n",
    "    # Number of non-NA values\n",
    "    num_non_na = dataframe['Wert'].count()\n",
    "\n",
    "    # Number of 0 values\n",
    "    num_zeros = np.sum(dataframe['Wert'] == 0)\n",
    "\n",
    "    if dataframe is df:\n",
    "        # Filter rows where 'Originalstatus' is not empty and not 'W'\n",
    "        filtered_data = df_original[df_original['Originalstatus'].notna()]\n",
    "    else:\n",
    "        filtered_data = dataframe[dataframe['Originalstatus'].notna()]\n",
    "\n",
    "    # Count the occurrences of each unique letter in 'Originalstatus'\n",
    "    letter_counts = filtered_data['Originalstatus'].value_counts()\n",
    "\n",
    "    # Descriptive statistics\n",
    "    mean_value = dataframe['Wert'].mean()\n",
    "    std_dev = dataframe['Wert'].std()\n",
    "    variance = dataframe['Wert'].var()\n",
    "    percentiles = dataframe['Wert'].quantile([0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "    min_value = dataframe['Wert'].min()\n",
    "    max_value = dataframe['Wert'].max()\n",
    "\n",
    "    # Create a dictionary with the statistics\n",
    "    statistics = {\n",
    "        \"Dataset\": [dataset],  # Use the provided name argument\n",
    "        \"Filename\": [filename],\n",
    "        \"ID\": [processed_files],\n",
    "        \"Pseudonym\": [alphabetic_id],\n",
    "        \"Timeseriesname\": [timeseries_name],\n",
    "        \"Number of rows\": [num_rows],\n",
    "        \"Number of non-NA values\": [num_non_na],\n",
    "        \"Number of 0 values\": [num_zeros],\n",
    "        \"Mean\": [mean_value],\n",
    "        \"Standard Deviation\": [std_dev],\n",
    "        \"Variance\": [variance],\n",
    "        \"Min Value\": [min_value],\n",
    "        \"Max Value\": [max_value]\n",
    "    }\n",
    "\n",
    "    # Add error counts by type for 'Dataset' only\n",
    "    for error_type, count in letter_counts.items():\n",
    "        column_name = f\"Error-Type: {error_type}\"\n",
    "        statistics[column_name] = [count]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    stats_df = pd.DataFrame(statistics)\n",
    "\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a6713c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id = 0: BQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  14%|█▍        | 1/7 [00:03<00:23,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id = 1: BT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  29%|██▊       | 2/7 [00:07<00:19,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file test folder\\Abfrage_hist_Werte_DOR2401012_20230929104910.csv: File test folder\\Abfrage_hist_Werte_DOR2401012_20230929104910.csv doesn't have 15-minute steps.\n",
      "id = 2: GA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  57%|█████▋    | 4/7 [00:09<00:05,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id = 3: DZO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  71%|███████▏  | 5/7 [00:10<00:03,  1.82s/it]C:\\Users\\UPBJVD\\AppData\\Local\\Temp\\ipykernel_9060\\2763680621.py:11: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, delimiter=';', decimal=',')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id = 4: FBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing files:  86%|████████▌ | 6/7 [00:12<00:01,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id = 5: FBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 7/7 [00:14<00:00,  2.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Filename</th>\n",
       "      <th>ID</th>\n",
       "      <th>Pseudonym</th>\n",
       "      <th>Timeseriesname</th>\n",
       "      <th>Number of rows</th>\n",
       "      <th>Number of non-NA values</th>\n",
       "      <th>Number of 0 values</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Min Value</th>\n",
       "      <th>Max Value</th>\n",
       "      <th>Error-Type: W</th>\n",
       "      <th>Error-Type: F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dataset_5</td>\n",
       "      <td>Dataset_FBM_5</td>\n",
       "      <td>5</td>\n",
       "      <td>FBM</td>\n",
       "      <td>UER4601903</td>\n",
       "      <td>195647</td>\n",
       "      <td>195647</td>\n",
       "      <td>11047</td>\n",
       "      <td>46.345727</td>\n",
       "      <td>12.318944</td>\n",
       "      <td>151.756382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.00</td>\n",
       "      <td>3140.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1_5</td>\n",
       "      <td>train_1_FBM_5</td>\n",
       "      <td>5</td>\n",
       "      <td>FBM</td>\n",
       "      <td>UER4601903</td>\n",
       "      <td>157632</td>\n",
       "      <td>157632</td>\n",
       "      <td>7560</td>\n",
       "      <td>46.966515</td>\n",
       "      <td>11.733341</td>\n",
       "      <td>137.671302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114.00</td>\n",
       "      <td>3140.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_1_5</td>\n",
       "      <td>test_1_FBM_5</td>\n",
       "      <td>5</td>\n",
       "      <td>FBM</td>\n",
       "      <td>UER4601903</td>\n",
       "      <td>35040</td>\n",
       "      <td>35040</td>\n",
       "      <td>317</td>\n",
       "      <td>47.681854</td>\n",
       "      <td>5.388750</td>\n",
       "      <td>29.038622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2_5</td>\n",
       "      <td>train_2_FBM_5</td>\n",
       "      <td>5</td>\n",
       "      <td>FBM</td>\n",
       "      <td>UER4601903</td>\n",
       "      <td>2975</td>\n",
       "      <td>2975</td>\n",
       "      <td>27</td>\n",
       "      <td>48.795745</td>\n",
       "      <td>5.370474</td>\n",
       "      <td>28.841988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_2_5</td>\n",
       "      <td>test_2_FBM_5</td>\n",
       "      <td>5</td>\n",
       "      <td>FBM</td>\n",
       "      <td>UER4601903</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dataset       Filename  ID Pseudonym Timeseriesname  Number of rows  \\\n",
       "0  Dataset_5  Dataset_FBM_5   5       FBM     UER4601903          195647   \n",
       "1  train_1_5  train_1_FBM_5   5       FBM     UER4601903          157632   \n",
       "2   test_1_5   test_1_FBM_5   5       FBM     UER4601903           35040   \n",
       "3  train_2_5  train_2_FBM_5   5       FBM     UER4601903            2975   \n",
       "4   test_2_5   test_2_FBM_5   5       FBM     UER4601903               0   \n",
       "\n",
       "   Number of non-NA values  Number of 0 values       Mean  Standard Deviation  \\\n",
       "0                   195647               11047  46.345727           12.318944   \n",
       "1                   157632                7560  46.966515           11.733341   \n",
       "2                    35040                 317  47.681854            5.388750   \n",
       "3                     2975                  27  48.795745            5.370474   \n",
       "4                        0                   0        NaN                 NaN   \n",
       "\n",
       "     Variance  Min Value  Max Value  Error-Type: W  Error-Type: F  \n",
       "0  151.756382        0.0     235.00         3140.0           16.0  \n",
       "1  137.671302        0.0     114.00         3140.0           16.0  \n",
       "2   29.038622        0.0     102.00            NaN            NaN  \n",
       "3   28.841988        0.0      66.05            NaN            NaN  \n",
       "4         NaN        NaN        NaN            NaN            NaN  "
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a counter for processed files\n",
    "processed_files = 0\n",
    "\n",
    "# Define the base file path\n",
    "base_path = \"Datasets/\"\n",
    "\n",
    "# Iterate through each file\n",
    "for id, file in tqdm(enumerate(file_list), total=total_files, desc=\"Processing files\"):\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file, delimiter=';', decimal=',')\n",
    "\n",
    "        # Convert 'Zeitstempel' column to datetime\n",
    "        df['Zeitstempel'] = pd.to_datetime(df['Zeitstempel'], format='%d.%m.%Y %H:%M')\n",
    "        \n",
    "        # Check if the time intervals are 15 minutes apart\n",
    "        time_diff = df['Zeitstempel'].diff()\n",
    "\n",
    "        # Print the time differences\n",
    "        if (time_diff > pd.Timedelta(\"1 days\")).any():\n",
    "            raise ValueError(f\"File {file} doesn't have 15-minute steps.\")\n",
    "\n",
    "        # Get timeseries_name\n",
    "        timeseries_name = df['Messstelle'].iloc[0]\n",
    "        for letter, timeseries in timeseries_mapping.items():\n",
    "            if timeseries_name == timeseries:\n",
    "                alphabetic_id = letter\n",
    "                print(f\"id = {processed_files}: {alphabetic_id}\")\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"No matching timeseries found for {timeseries_name}\")\n",
    "\n",
    "        # Assign df to df_original\n",
    "        df_original = df.copy()  # Create a copy to avoid modifying df\n",
    "        df_original\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "        continue \n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------    \n",
    "## Step 1: Insert original values into timeseries and mark the right status \n",
    "\n",
    "    # Find rows where \"Originalwert\" is not NaN and drop the columns afterwards\n",
    "    if 'Originalwert' in df.columns and 'Originalstatus' in df.columns:\n",
    "        mask = ~df['Originalwert'].isna() #gives True or False\n",
    "\n",
    "        # Replace \"Wert\" with \"Originalwert\" where \"Originalwert\" is not NaN (loc gives u the complete row)\n",
    "        df.loc[mask, 'Wert'] = df.loc[mask, 'Originalwert'] \n",
    "\n",
    "        # Set \"Status\" at index 0 to 0 and all \"W\"s in \"Status\" to 1\n",
    "        df.loc[df['Status'] == 'W', 'Status'] = 1\n",
    "        df.loc[df['Status'] == 'E', 'Status'] = 0\n",
    "\n",
    "        df = df.drop(['Originalwert', 'Originalstatus'], axis=1)\n",
    "    else:\n",
    "        print(\"Columns 'Originalwert' and 'Originalstatus' do not exist in the DataFrame.\")\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------    \n",
    "## Step 2: Split Dataset into training and test set\n",
    "\n",
    "    # Define the conditions for each dataset\n",
    "    train_1_condition = (df['Zeitstempel'] >= train_1_start) & (df['Zeitstempel'] <= train_1_end)\n",
    "    test_1_condition = (df['Zeitstempel'] >= test_1_start) & (df['Zeitstempel'] <= test_1_end)\n",
    "    train_2_condition = (df['Zeitstempel'] >= train_2_start) & (df['Zeitstempel'] <= train_2_end)\n",
    "    test_2_condition = (df['Zeitstempel'] >= test_2_start) & (df['Zeitstempel'] <= test_2_end)\n",
    "\n",
    "    # Create the datasets\n",
    "    train_1 = df[train_1_condition]\n",
    "    test_1 = df[test_1_condition]\n",
    "    train_2 = df[train_2_condition]\n",
    "    test_2 = df[test_2_condition]\n",
    "\n",
    "    # Reset the index for the new datasets\n",
    "    train_1 = train_1.reset_index(drop=True)\n",
    "    test_1 = test_1.reset_index(drop=True)\n",
    "    train_2 = train_2.reset_index(drop=True)\n",
    "    test_2 = test_2.reset_index(drop=True)\n",
    "    \n",
    "    train_1_original =  df_original[train_1_condition]\n",
    "    test_1_original = df_original[test_1_condition]\n",
    "    train_2_original = df_original[train_2_condition]\n",
    "    test_2_original = df_original[test_2_condition]\n",
    "    \n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------    \n",
    "## Step 3: Standardize training set and see if the first trainingset is available\n",
    "\n",
    "    if train_1.empty is False: \n",
    "        #Get Statistics\n",
    "        mean_wert_train_1 = train_1_original['Wert'].mean()\n",
    "        mean_wert_train_2 = train_2_original['Wert'].mean()\n",
    "\n",
    "        #Scale the data\n",
    "        train_1['Wert'] = train_1['Wert'].apply(lambda x: 0 if x == 0 else round(x / mean_wert_train_1, 7))\n",
    "        train_2['Wert'] = train_2['Wert'].apply(lambda x: 0 if x == 0 else round(x / mean_wert_train_2, 7))\n",
    "\n",
    "        ## Step 4: Apply Standardization to test set\n",
    "\n",
    "        # Scale the data\n",
    "        test_1['Wert'] = test_1['Wert'].apply(lambda x: 0 if x == 0 else round(x / mean_wert_train_1, 7))\n",
    "        test_2['Wert'] = test_2['Wert'].apply(lambda x: 0 if x == 0 else round(x / mean_wert_train_2, 7))\n",
    "        \n",
    "        ## Step 5: Store train and testset and make a list for remaining datasets\n",
    "        \n",
    "        # Copy relevant columns\n",
    "        train_1_subset = train_1[['Zeitstempel','Wert', 'Status']].copy()\n",
    "        test_1_subset = test_1[['Zeitstempel','Wert', 'Status']].copy()\n",
    "        train_2_subset = train_2[['Zeitstempel','Wert', 'Status']].copy()\n",
    "        test_2_subset = test_2[['Zeitstempel','Wert', 'Status']].copy()\n",
    "        \n",
    "        #Merge empty df with the subsets to get the whole dataset\n",
    "        train_1_merged = pd.merge(train_1_empty_df, train_1_subset, on='Zeitstempel', how='outer')\n",
    "        train_1_merged = train_1_merged.drop(['Zeitstempel'], axis=1)\n",
    "        \n",
    "        test_1_merged = pd.merge(test_1_empty_df, test_1_subset, on='Zeitstempel', how='outer')\n",
    "        test_1_merged = test_1_merged.drop(['Zeitstempel'], axis=1)\n",
    "        \n",
    "        train_2_merged = pd.merge(train_2_empty_df, train_2_subset, on='Zeitstempel', how='outer')\n",
    "        train_2_merged = train_2_merged.drop(['Zeitstempel'], axis=1)\n",
    "        \n",
    "        test_2_merged = pd.merge(test_2_empty_df, test_2_subset, on='Zeitstempel', how='outer')\n",
    "        test_2_merged = test_2_merged.drop(['Zeitstempel'], axis=1)\n",
    "\n",
    "        datasets_original = [df, train_1_original, test_1_original, train_2_original, test_2_original]\n",
    "        datasets = [ train_1_merged, test_1_merged, train_2_merged, test_2_merged]\n",
    "\n",
    "        # Define the subfolders and corresponding DataFrame names\n",
    "        subfolders = ['train_1', 'test_1', 'train_2', 'test_2']\n",
    "\n",
    "        # Combine subfolders with corresponding DataFrames\n",
    "        folders_data = zip(subfolders, datasets)\n",
    "\n",
    "        # Iterate over the combined data\n",
    "        for folder, data in folders_data:\n",
    "            # Define the file path\n",
    "            file_path = f\"{base_path}{folder}/{folder}_{alphabetic_id}_{processed_files}.csv\"\n",
    "\n",
    "            # Export the DataFrame to a CSV file\n",
    "            data.to_csv(file_path, sep=';', decimal=',', index=False)\n",
    "        \n",
    "    else:\n",
    "        test_1.drop(test_1.index, inplace=True)\n",
    "        mean_wert_train_2 = train_2_original['Wert'].mean()\n",
    "        train_2['Wert'] = train_2['Wert'].apply(lambda x: 0 if x == 0 else round(x / mean_wert_train_2, 7))\n",
    "        \n",
    "        ## Step 4: Apply Standardization to test set\n",
    "\n",
    "        # Scale the data\n",
    "        test_2['Wert'] = test_2['Wert'].apply(lambda x: 0 if x == 0 else round(x / mean_wert_train_2, 7))\n",
    "        \n",
    "        ## Step 5: Store train and testset and make a list for remaining datasets\n",
    "        \n",
    "        # Copy relevant columns\n",
    "        train_2_subset = train_2[['Zeitstempel','Wert', 'Status']].copy()\n",
    "        test_2_subset = test_2[['Zeitstempel','Wert', 'Status']].copy()\n",
    "        \n",
    "        #Merge empty df with the subsets to get the whole dataset    \n",
    "        train_2_merged = pd.merge(train_2_empty_df, train_2_subset, on='Zeitstempel', how='outer')\n",
    "        train_2_merged = train_2_merged.drop(['Zeitstempel'], axis=1)\n",
    "        \n",
    "        test_2_merged = pd.merge(test_2_empty_df, test_2_subset, on='Zeitstempel', how='outer')\n",
    "        test_2_merged = test_2_merged.drop(['Zeitstempel'], axis=1)\n",
    "        \n",
    "        datasets_original = [df, train_2_original, test_2_original]\n",
    "        datasets = [train_2_merged, test_2_merged]\n",
    "\n",
    "        # Define the subfolders and corresponding DataFrame names\n",
    "        subfolders = ['train_2', 'test_2']\n",
    "\n",
    "        # Combine subfolders with corresponding DataFrames\n",
    "        folders_data = zip(subfolders, datasets)\n",
    "\n",
    "        # Iterate over the combined data\n",
    "        for folder, data in folders_data:\n",
    "            # Define the file path\n",
    "            file_path = f\"{base_path}{folder}/{folder}_{alphabetic_id}_{processed_files}.csv\"\n",
    "\n",
    "            # Export the DataFrame to a CSV file\n",
    "            data.to_csv(file_path, sep=';', decimal=',', index=False)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------    \n",
    "## Step 7: Get Statistics from the whole dataset \n",
    "    final_stats_df = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "\n",
    "    for dataset in datasets_original:\n",
    "        stats_df = generate_statistics(dataset)\n",
    "        final_stats_df = pd.concat([final_stats_df, stats_df])\n",
    "\n",
    "    # Reset the index of the final DataFrame\n",
    "    final_stats_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save the resulting DataFrame\n",
    "    file_path_stats = f\"Statistics/statistics_{alphabetic_id}_{processed_files}.csv\"\n",
    "    \n",
    "    # Export the DataFrame to a CSV file\n",
    "    final_stats_df.to_csv(file_path_stats, sep=';', decimal=',', index = False)\n",
    "\n",
    "    # Increment the counter for processed files\n",
    "    processed_files += 1\n",
    "    \n",
    "final_stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21356da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
